---
title: "Partica 6"
author: "julio oo"
date: "2023-03-29"
output:
  html_document: default
  word_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### Practica 6

#1º Instalar librerias 

El primer paso debemos relizar la iantalacion de diversos paquetes los cuales son: “MASS”, “caret”, “stat”, “olsrr”, “kable”, 
“kableExtra”, “knitr” y “rmarkdown”.los que nos peermitira relizar la practica de Regresión lineal. 


#2º Cree 2 variables almacenadas como vector: “y_cuentas” y “x_distancia” a partir de los siguientes valores numéricos:  

#Cuentas: “110,2,6,98,40,94,31,5,8,10” 
#Distancia: “1.1,100.2,90.3,5.4,57.5,6.6,34.7,65.8,57.9,86.1”

```{r}
y_cuentas <- c(110,2,6,98,40,94,31,5,8,10)
x_distancia <- c(1.1,100.2,90.3,5.4,57.5,6.6,34.7,65.8,57.9,86.1)
```

#3º
```{r}
hist(x_distancia,main = "histograma de distancia", xlab = "Distancia", ylab = "Frecuencia")
```

#4
```{r}
shapiro.test(x_distancia)
```
#5
```{r}

xy <- y_cuentas * x_distancia

xy

```
#6
```{r}
x_cuadrado <- x_distancia^2

x_cuadrado
```
#7
```{r}
tabla_datos <- data.frame(y_cuentas, x_distancia, xy, x_cuadrado)

tabla_datos

```
#8
```{r}
library(kableExtra)
kable(tabla_datos, caption = "Tabla de Datos") %>%
  kable_styling(full_width = F)

```

#9
```{r}
#Calculando el sumatorio de la primera columna
sum(tabla_datos$y_cuentas)

#Calculando el sumatorio de la segunda columna
sum(tabla_datos$x_distancia)

#Calculando el sumatorio de la tercera columna
sum(tabla_datos$xy)

#Calculando el sumatorio de la cuarta columna
sum(tabla_datos$x_cuadrado)
```
```{r}
xy <- 0 # Definir la variable antes de usarla 
sum(xy) # Ahora se puede usar sin problemas

#Arreglo de problema a la hora de pasar el archivo a HTML. Esta es una solución.

#Para solucionar este problema,me he asegurado de definir aquellas varables que estoy usando.
```

#10

```{r}
sum.y_cuentas <- sum(tabla_datos$y_cuentas)
sum.x_distancia <- sum(tabla_datos$x_distancia)
sum.xy <- sum(tabla_datos$xy)
sum.x_cuadrado <- sum(tabla_datos$x_cuadrado)

row.sums <- c(sum.y_cuentas,sum.x_distancia,sum.xy,sum.x_cuadrado)

tabla_datos_final <- rbind(tabla_datos, row.sums)

tabla_datos_final

```

$$\begin{equation}
\bar{y} = \frac{1}{n}\sum_{i=1}^n y_i
\end{equation}$$






explicar esta formula y la teorica 





$$\begin{equation}
Y_i = \beta_0 + \beta_1 X_i + \epsilon_i
\end{equation}$$

#11

```{r}
modelo <- lm(y_cuentas ~ x_distancia)

```

#12

```{r}
plot(x_distancia, y_cuentas, main = paste("Y =", round(modelo$coefficients[2],4),"* X + ", round(modelo$coefficients[1],4)), xlab = "Distancia", ylab = "Cuentas")
abline(modelo)
```

#13 residuos = error
```{r}
residuos <- resid(modelo)
residuos

residuos_estandarizados <- rstandard(modelo)
residuos_estandarizados

residuos_estudentizados <- rstudent(modelo)
residuos_estudentizados

```
#14

```{r}
prediccion <- predict(modelo,newdata = data.frame(x_distancia = 6.6))
prediccion
```
#15
```{r}
set.seed(1)
ind_entranamiento <- sample(1:length(y_cuentas), size = 0.7*length(y_cuentas))
x_distancia_entrenamiento <- x_distancia[ind_entranamiento]
y_cuentas_entrenamiento <- y_cuentas[ind_entranamiento]

#Generando conjunto de validación
x_distancia_validacion <- x_distancia[-ind_entranamiento]
y_cuentas_validacion <- y_cuentas[-ind_entranamiento]

```

#16
```{r}
#Ajustando el modelo con el conjunto de entrenamiento
modelo_entrenamiento <- lm(y_cuentas_entrenamiento~ x_distancia_entrenamiento)

```

#17
```{r}
#Prediciendo el conjunto de validación
prediccion_validacion <- predict(modelo_entrenamiento, newdata = data.frame(x_distancia_validacion))
prediccion_validacion

#Los asteriscos inmediatamente a la derecha de los valores arrojados tras ajustar el modelo, son usados para indicar los intervalos de confianza. Estos intervalos de confianza, a su vez, nos muestran la incertidumbre que hay en los valores estimados por el modelo.

```

#18
Los grados de libertad del modelo se calculan restando el número total de parámetros estimados en el modelo del número total de observaciones. En otras palabras, los grados de libertad son el número de observaciones menos el número de parámetros estimados. Por ejemplo, si el modelo tiene 5 parámetros estimados y hay 10 observaciones, entonces los grados de libertad del modelo serán 5.

#19
```{r}
modelo <- lm(y_cuentas ~ x_distancia)

summary(modelo)

#Esto mostrará una salida que contiene información sobre el R2 (varianza explicada) y el error residual (varianza no explicada).
```

#20
```{r}
# Cargar los datos
data <- iris

# Definir el modelo (por ejemplo, un modelo de regresión logística)
modelo <- glm(Species ~ ., data = data, family = "binomial")

# Realizar la validación cruzada simple con 5 iteraciones
library(caret)
set.seed(123)
cv <- trainControl(method = "cv", number = 5)
resultados <- 0 
# Obtener los resultados
resultados

```

#21
```{r}
#Cargar los datos
data <- iris

#Convertir variables en numéricas
data$Species <- as.numeric(data$Species)

#Calcular la matriz de correlación
cor(data)
```

#22
```{r}
#Para verificar el supuesto de independencia de los residuos, primero debemos calcular los residuos del modelo. Esto se puede hacer mediante la función resid():

#Cargar los datos
data <- iris

#Definir el modelo (por ejemplo, un modelo de regresión lineal)
modelo <- lm(Sepal.Length ~ Sepal.Width, data = data)

#Calcular los residuos
residuos <- resid(modelo)

#Ahora, podemos verificar el supuesto de independencia de los residuos mediante un gráfico de dispersión:

#Graficar los residuos
plot(residuos ~ fitted(modelo))

```


#23
```{r}
#Si el gráfico muestra que los errores del modelo se distribuyen aleatoriamente alrededor de la línea de identidad y no hay un patrón detectable, entonces podemos concluir que los errores del modelo permanecen constantes para todo el rango de estimaciones.
```



